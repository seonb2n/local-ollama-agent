"""
Ollama ì—°ê²° ë° ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import requests
import json
import time

# Ollama API ì„¤ì •
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "deepseek-coder-v2:16b-lite-instruct-q4_K_M"


def test_ollama_connection():
    """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags")
        if response.status_code == 200:
            models = response.json()
            print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ!")
            print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
            for model in models.get('models', []):
                print(f"  - {model['name']}")
            return True
        else:
            print(f"âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def test_model_generation():
    """ëª¨ë¸ ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¤– ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {MODEL_NAME}")

    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    prompt = """
íŒŒì´ì¬ìœ¼ë¡œ ì£¼ì‹ ê°€ê²©ì„ ì¡°íšŒí•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
í•¨ìˆ˜ëª…ì€ get_stock_priceì´ê³ , ì¢…ëª© ì½”ë“œë¥¼ ì…ë ¥ë°›ì•„ì„œ í˜„ì¬ê°€ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
yfinance ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
"""

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    }

    try:
        print("â³ ì½”ë“œ ìƒì„± ì¤‘... (30ì´ˆ-2ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        start_time = time.time()

        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json=payload,
            timeout=120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        end_time = time.time()

        if response.status_code == 200:
            result = response.json()
            generated_code = result.get('response', '')

            print(f"âœ… ì½”ë“œ ìƒì„± ì„±ê³µ! (ì†Œìš”ì‹œê°„: {end_time - start_time:.1f}ì´ˆ)")
            print("\n" + "=" * 50)
            print("ğŸ“ ìƒì„±ëœ ì½”ë“œ:")
            print("=" * 50)
            print(generated_code)
            print("=" * 50)
            return True
        else:
            print(f"âŒ ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {response.status_code}")
            print(f"ì‘ë‹µ: {response.text}")
            return False

    except requests.exceptions.Timeout:
        print("â° ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (2ë¶„). ëª¨ë¸ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ ì½”ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        return False


def test_langchain_integration():
    """LangChain ì—°ë™ í…ŒìŠ¤íŠ¸"""
    try:
        from langchain_community.llms import Ollama

        print(f"\nğŸ”— LangChain ì—°ë™ í…ŒìŠ¤íŠ¸")

        # LangChain Ollama ê°ì²´ ìƒì„±
        llm = Ollama(model=MODEL_NAME)

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        prompt = "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."

        print("â³ LangChainìœ¼ë¡œ ì½”ë“œ ìƒì„± ì¤‘...")
        start_time = time.time()

        response = llm.invoke(prompt)

        end_time = time.time()

        print(f"âœ… LangChain ì—°ë™ ì„±ê³µ! (ì†Œìš”ì‹œê°„: {end_time - start_time:.1f}ì´ˆ)")
        print("\n" + "=" * 30)
        print("ğŸ“ LangChain ì‘ë‹µ:")
        print("=" * 30)
        print(response)
        print("=" * 30)
        return True

    except ImportError:
        print("âŒ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹: pip install langchain langchain-community")
        return False
    except Exception as e:
        print(f"âŒ LangChain ì—°ë™ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“ ëŒ€ìƒ ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸŒ Ollama URL: {OLLAMA_URL}")
    print("-" * 60)

    # 1. ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_ollama_connection():
        print("\nâŒ Ollama ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # 2. ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
    if not test_model_generation():
        print("\nâŒ ëª¨ë¸ ì½”ë“œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return

    # 3. LangChain ì—°ë™ í…ŒìŠ¤íŠ¸
    if not test_langchain_integration():
        print("\nâŒ LangChain ì—°ë™ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return

    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("âœ… Ollama ì„œë²„ ì—°ê²°ë¨")
    print("âœ… ëª¨ë¸ ì½”ë“œ ìƒì„± ì‘ë™í•¨")
    print("âœ… LangChain ì—°ë™ ì„±ê³µ")
    print("\në‹¤ìŒ ë‹¨ê³„: FastAPI ì—ì´ì „íŠ¸ êµ¬í˜„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()